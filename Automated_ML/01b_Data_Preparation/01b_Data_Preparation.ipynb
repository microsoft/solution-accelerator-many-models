{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/manymodels/01_Data_Preparation/01_Data_Preparation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01b Data Preparation \n",
    "This notebook uses simulated orange juice sales data to walk you through the process of training many models on Azure Machine Learning using Automated ML. \n",
    "\n",
    "The time series data used in this example was simulated based on the University of Chicago's Dominick's Finer Foods dataset which featured two years of sales of 3 different orange juice brands for individual stores. The full simulated dataset includes 3,991 stores with 3 orange juice brands each thus allowing 11,973 models to be trained to showcase the power of the many models pattern.\n",
    "\n",
    "  \n",
    "In this notebook, two datasets will be created: one with all 11,973 files and one with only 10 files that can be used to quickly test and debug. For each dataset, you'll be walked through the process of:\n",
    "\n",
    "1. Registering the blob container as a Datastore to the Workspace\n",
    "2. Registering a File Dataset to the Workspace\n",
    "\n",
    "### Prerequisites\n",
    "If you have already run the [00_Setup_AML_Workspace](../../00_Setup_AML_Workspace.ipynb) notebooks you are all set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Connect to your Workspace and Datastore\n",
    "In the configuration notebook you created a [Workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py). We are going to use that enviroment to register the data. You also set up the Datastore which in this example is a container in Blob storage where we will store the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config() \n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Take a look at Workspace\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data Preparation\n",
    "The OJ data is available in the public blob container. The data is split to be used for training and for inferencing. For the current dataset, the data was split on time column ('WeekStarting') before and after '1992-5-28' .\n",
    "\n",
    "The container has 'oj_data' and 'oj_inference' folders that contains training and inference data respectively for the 11,973 models. It also has 'oj_data_small' and 'oj_inference_small' folders that has training and inference data for 10 models.\n",
    "\n",
    "To create the [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py) needed for the ParallelRunStep, you first need to register the blob container to the workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> To use your own data, create a local folder with each group as a separate file. The data has to be presplit into different files by group and for OJ data, it was already split by Store and Brand. For timeseries, the groups must not split up individual timeseries. That is, each group must contain one or more whole time-series</b>\n",
    "Then use that folder and directory in section 3.0 to upload your data to the Datastore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Register the blob container as DataStore\n",
    "\n",
    "A Datastore is a place where data can be stored that is then made accessible to a compute either by means of mounting or copying the data to the compute target.\n",
    "\n",
    "Please refer to [Datastore](https://docs.microsoft.com/python/api/azureml-core/azureml.core.datastore(class)) documentation on how to access data from Datastore.\n",
    "\n",
    "In this next step, we will be registering blob storage as datastore to the Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "# Please change the following to point to your own blob container and pass in account_key\n",
    "blob_datastore_name = \"automl_many_models\"\n",
    "container_name = \"automl-sample-notebook-data\"\n",
    "account_name = \"automlsamplenotebookdata\"\n",
    "\n",
    "oj_datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                       datastore_name=blob_datastore_name, \n",
    "                                                       container_name=container_name,\n",
    "                                                       account_name=account_name,\n",
    "                                                       create_if_not_exists=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Create the FileDatasets \n",
    "\n",
    "Now that the datastore is available from the Workspace, [FileDatasets](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py) can be created. Datasets in Azure Machine Learning are references to specific data in a Datastore.  We are using FileDatasets since we are storing each group as a separate file. This will help to separate the data needed for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "ds_name = 'oj_data'\n",
    "input_ds = Dataset.File.from_files(path=oj_datastore.path(ds_name + '/'), validate=False)\n",
    "\n",
    "inference_name = 'oj_inference'\n",
    "inference_ds = Dataset.File.from_files(path=oj_datastore.path(inference_name + '/'), validate=False)\n",
    "\n",
    "ds_name_small = 'oj_data_small'\n",
    "input_ds_small = Dataset.File.from_files(path=oj_datastore.path(ds_name_small + '/'), validate=False)\n",
    "\n",
    "inference_name_small = 'oj_inference_small'\n",
    "inference_ds_small = Dataset.File.from_files(path=oj_datastore.path(inference_name_small + '/'), validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Register the FileDataSets to the Workspace \n",
    "Finally, register the dataset to your Workspace so it can be called as an input into the training pipeline in the next notebook. We will use the inference dataset as part of the forecasting pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ds.register(ws, ds_name, create_new_version=True)\n",
    "inference_ds.register(ws, inference_name, create_new_version=True)\n",
    "\n",
    "input_ds_small.register(ws, ds_name_small, create_new_version=True)\n",
    "inference_ds_small.register(ws, inference_name_small, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Call the Registered dataset *(Optional)*\n",
    "After registering the data, it can be easily called using the command below. This is how the datasets will be accessed in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oj_ds = Dataset.get_by_name(ws, name = ds_name_small)\n",
    "oj_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also download the data from the dataset in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oj_ds.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "Now that you have created your dataset, you are ready to move to the Training Notebook to create models. "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "deeptim"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
