{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Setup\n",
    "---\n",
    "\n",
    "This notebook walks you through all the necessary steps to configure your environment and data for this solution accelerator including:\n",
    "\n",
    "1. Connect to your workspace\n",
    "2. Deploying a compute cluster for training and forecasting\n",
    "3. Creating and registering the dataset used in this accelerator\n",
    "\n",
    "### Prerequisites\n",
    "If you have already run the [00_Setup_AML_Workspace](../00_Setup_AML_Workspace.ipynb) notebooks you are all set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Connect to your Workspace\n",
    "In the [00_Setup_AML_Workspace](../00_Setup_AML_Workspace.ipynb) notebook you created a [Workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config() \n",
    "\n",
    "\n",
    "# Take a look at Workspace\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Create compute\n",
    "\n",
    "In this step we create an compute cluster that will be used for the training and forecasting pipelines. This is a one-time set up so you won't need to re-run this in future notebooks.\n",
    "\n",
    "We create a STANDARD_D13_V2 compute cluster. D-series VMs are used for tasks that require higher compute power and temporary disk performance. This [page](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs) will gives you more information on VM sizes to help you decide which will best fit your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D13_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=5)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Create dataset\n",
    "\n",
    "This solution accelerator uses simulated orange juice sales data from [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/) to walk you through the process of training many models on Azure Machine Learning. You can learn more about the dataset [here](https://azure.microsoft.com/en-us/services/open-datasets/catalog/sample-oj-sales-simulated/). The full dataset includes simulared sales for 3,991 stores with 3 orange juice brands each thus allowing 11,973 models to be trained to showcase the power of the many models pattern.\n",
    "\n",
    "We'll start by downloading the first 10 files but you can easily edit the code below to train all 11,973 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade azureml-opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.opendatasets import OjSalesSimulated\n",
    "\n",
    "# Pull all of the data\n",
    "oj_sales_files = OjSalesSimulated.get_file_dataset()\n",
    "\n",
    "# Pull only the first 10 files\n",
    "oj_sales_files_small = OjSalesSimulated.get_file_dataset().take(10)\n",
    "\n",
    "# Create a folder to download\n",
    "target_path = 'oj_sales_data' \n",
    "if not os.path.exists(target_path):\n",
    "    os.mkdir(target_path)\n",
    "\n",
    "# Download the data\n",
    "oj_sales_files_small.download(target_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create and register a [dataset](https://docs.microsoft.com/en-us/azure/machine-learning/concept-data#datasets) in Azure Machine Learning. \n",
    "\n",
    "Using a [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py) is currently the best way to take advantage of the many models pattern so we create a FileDataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Upload the data\n",
    "datastore.upload(src_dir = target_path,\n",
    "                target_path = target_path,\n",
    "                overwrite = True)\n",
    "\n",
    "# Create a file dataset\n",
    "path_on_datastore = datastore.path(target_path)\n",
    "ds = Dataset.File.from_files(path=path_on_datastore, validate=False)\n",
    "\n",
    "# Register the file dataset\n",
    "dataset_name = 'oj_data_small'\n",
    "ds.register(ws, dataset_name, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've set up your workspace and created a dataset, move on to 02_Training_Pipeline.ipynb to train and score the models."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
