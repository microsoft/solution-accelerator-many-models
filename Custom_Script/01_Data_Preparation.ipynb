{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Setup\n",
    "---\n",
    "\n",
    "This notebook walks you through all the necessary steps to configure your environment and data for this solution accelerator including:\n",
    "\n",
    "1. Connect to your workspace\n",
    "2. Deploying a compute cluster for training and forecasting\n",
    "3. Create, split, and register Datasets used in this accelerator\n",
    "\n",
    "### Prerequisites\n",
    "If you have already run the [00_Setup_AML_Workspace](../00_Setup_AML_Workspace.ipynb) notebooks you are all set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Connect to your Workspace\n",
    "In the [00_Setup_AML_Workspace](../00_Setup_AML_Workspace.ipynb) notebook you created a [Workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config() \n",
    "\n",
    "# Take a look at Workspace\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Create compute\n",
    "\n",
    "In this step we create an compute cluster that will be used for the training and forecasting pipelines. This is a one-time set up so you won't need to re-run this in future notebooks.\n",
    "\n",
    "We create a STANDARD_D13_V2 compute cluster. D-series VMs are used for tasks that require higher compute power and temporary disk performance. This [page](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs) will gives you more information on VM sizes to help you decide which will best fit your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D13_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=5)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Create Datasets\n",
    "\n",
    "This solution accelerator uses simulated orange juice weekly sales data from [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/) to walk you through the process of training many models on Azure Machine Learning. You can learn more about the dataset [here](https://azure.microsoft.com/en-us/services/open-datasets/catalog/sample-oj-sales-simulated/). \n",
    "\n",
    "The full dataset includes simulared sales for 3,991 stores with 3 orange juice brands each thus allowing 11,973 models to be trained to showcase the power of the many models pattern. Each series contains data from '1990-06-14' to '1992-10-01'.\n",
    "\n",
    "We'll start by downloading the first 10 files but you can easily edit the code below to train all 11,973 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade azureml-opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.opendatasets import OjSalesSimulated\n",
    "\n",
    "# Pull all of the data\n",
    "oj_sales_files = OjSalesSimulated.get_file_dataset()\n",
    "\n",
    "# Pull only the first 10 files\n",
    "oj_sales_files_small = OjSalesSimulated.get_file_dataset().take(10)\n",
    "\n",
    "# Create a folder to download\n",
    "target_path = 'oj_sales_data' \n",
    "if not os.path.exists(target_path):\n",
    "    os.mkdir(target_path)\n",
    "\n",
    "# Download the data\n",
    "oj_sales_files_small.download(target_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split each dataset in two parts: one will be used for training in the [training notebook](02_Training_Pipeline.ipynb), and the other will be used for simulating batch inferencing in the [forecasting notebook](03_Forecasting_Pipeline.ipynb). The training files will contain the data records before '1992-5-28' and the last part of each series will be stored in the inferencing files.\n",
    "\n",
    "Finally, we will upload both sets of data files to the Workspace's default [Datastore](https://docs.microsoft.compython/api/azureml-core/azureml.core.datastore(class))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helper import split_data_upload_to_datastore\n",
    "\n",
    "# Connect to default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Set upload paths for train and inference splits\n",
    "ds_train_path = target_path + '_train'\n",
    "ds_inference_path = target_path + '_inference'\n",
    "\n",
    "# Provide name of timestamp column in the data and date from which to split into the inference dataset\n",
    "timestamp_column = 'WeekStarting'\n",
    "split_date = '1992-05-28'\n",
    "\n",
    "# Split each file and upload both sets to the datastore \n",
    "split_data_upload_to_datastore(target_path, timestamp_column, split_date, datastore, ds_train_path, ds_inference_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create and register [datasets](https://docs.microsoft.com/en-us/azure/machine-learning/concept-data#datasets) in Azure Machine Learning for the train and inference sets. \n",
    "\n",
    "Using a [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py) is currently the best way to take advantage of the many models pattern so we create FileDatasets in the next cell. We also [register](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets#register-datasets) the Datasets in your Workspace; this associates the train/inference sets with simple names that can be easily referred to later on when we train models and produce forecasts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file datasets\n",
    "ds_train = Dataset.File.from_files(path=datastore.path(ds_train_path), validate=False)\n",
    "ds_inference = Dataset.File.from_files(path=datastore.path(ds_inference_path), validate=False)\n",
    "\n",
    "# Register the file datasets\n",
    "dataset_name = 'oj_data_small'\n",
    "train_dataset_name = dataset_name + '_train'\n",
    "inference_dataset_name = dataset_name + '_inference'\n",
    "ds_train.register(ws, train_dataset_name, create_new_version=True)\n",
    "ds_inference.register(ws, inference_dataset_name, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've set up your Workspace and created Datasets, move on to 02_Training_Pipeline.ipynb to train and score the models."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
