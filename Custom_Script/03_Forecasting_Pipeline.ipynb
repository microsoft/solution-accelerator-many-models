{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Pipeline\n",
    "---\n",
    "\n",
    "In this notebook we create a pipeline to forecast sales with the models we trained in the last step. The forecasting pipeline we'll set up is similar to the training pipeline in the last step so we'll keep the documentation light. For more details on the steps and functions refer to the last notebook.\n",
    "\n",
    "### Prerequisites\n",
    "At this point, you should have already:\n",
    "\n",
    "1. Created your AML Workspace using the [00_Setup_AML_Workspace notebook](../00_Setup_AML_Workspace.ipynb)\n",
    "2. Run [01_Data_Preparation.ipynb](01_Data_Preparation.ipynb) to setup your compute and create the dataset\n",
    "3. Run [02_Training_Pipeline.ipynb](02_Training_Pipeline.ipynb) to train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Connect to workspace and datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core import Datastore\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# set up datastores\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "print('Workspace Name: ' + ws.name, \n",
    "      'Azure Region: ' + ws.location, \n",
    "      'Subscription Id: ' + ws.subscription_id, \n",
    "      'Resource Group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Create an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'forecasting_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Get the dataset\n",
    "Now we get a reference to the test data in our Datastore. We will use the models trained in the Modeling Notebook to generate forecasts over all rows in each test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "small_dataset = Dataset.get_by_name(ws, name='oj_data_small_test')\n",
    "small_dataset_input = small_dataset.as_named_input('forecast_10_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Create ParallelRunStep for the forecasting pipeline\n",
    "As we did with the training pipeline, we'll create a ParallelRunStep to parallelize our forecasting process. You'll notice this code is essentially the same as the last step except that we'll be parallelizing [**forecast.py**](scripts/forecast.py) rather than train.py. Note that we still need to pass the timeseries schema (timestamp column name, timeseries ID column names, etc) to the forecasting script. Unlike the training script, however, the name of target column is not required for the forecasting script, just optional. In a true forecasting scenario the actual values of the target are not available, of course, so the forecasting pipeline would just return predictions. In a testing scenario, the forecasting pipeline can also return the actuals, if the column name is provided, so that we may evaluate the accuracy of the forecasts on a hold-out set.\n",
    "\n",
    "### 4.1 Configure environment for ParallelRunStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "forecast_env = Environment(name=\"many_models_environment\")\n",
    "forecast_conda_deps = CondaDependencies.create(pip_packages=['sklearn', 'joblib'])\n",
    "forecast_env.python.conda_dependencies = forecast_conda_deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Choose a compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "compute = AmlCompute(ws, \"cpucluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Set up ParallelRunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunConfig \n",
    "\n",
    "process_count_per_node = 6\n",
    "node_count = 1\n",
    "timeout = 180\n",
    "\n",
    "tags = {}\n",
    "tags['node_count'] = node_count\n",
    "tags['process_count_per_node'] = process_count_per_node\n",
    "tags['timeout'] = timeout\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./scripts',\n",
    "    entry_script='forecast.py',\n",
    "    mini_batch_size='1',\n",
    "    run_invocation_timeout=timeout, \n",
    "    error_threshold=10,\n",
    "    output_action='append_row', \n",
    "    environment=forecast_env, \n",
    "    process_count_per_node=process_count_per_node, \n",
    "    compute_target=compute, \n",
    "    node_count=node_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Set up ParallelRunStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep \n",
    "\n",
    "output_dir = PipelineData(name='forecasting_output', datastore=dstore)\n",
    "\n",
    "parallel_run_step = ParallelRunStep(\n",
    "    name=\"many-models-forecasting\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[small_dataset_input],\n",
    "    output=output_dir,\n",
    "    allow_reuse=False,\n",
    "    arguments=['--target_column', 'Quantity',  # Since this is a testing scenario, pass the target column name\n",
    "               '--timestamp_column', 'WeekStarting',\n",
    "               '--timeseries_id_columns', 'Store', 'Brand',\n",
    "               '--model_type', 'lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Create step to copy predictions\n",
    "\n",
    "The forecasting pipeline includes a second step that copies the predictions from *parallel_run_step.txt* to a CSV file in a separate container. While this step is simple, it demonstates how a step can be added to the pipeline to upload the predictions to a separate datastore or make additional transformations to the output.\n",
    "\n",
    "### 5.1 Create a data reference\n",
    "First, we create a datastore named **predictions** to hold the outputs of the pipeline and get a reference to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "output_dstore = Datastore.register_azure_blob_container(\n",
    "    workspace=ws, \n",
    "    datastore_name=\"predictions\",\n",
    "    container_name=\"predictions\",\n",
    "    account_name=dstore.account_name,\n",
    "    account_key=dstore.account_key,\n",
    "    create_if_not_exists=True\n",
    ")\n",
    "\n",
    "output_dref = DataReference(output_dstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create PythonScriptStep\n",
    "Next, we define the [PythonScriptStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py) and give it our newly create datastore as well as the location of the *parallel_run_step.txt*. Note that the copy script also uses the timeseries schema; the reason is that the copy script creates a header row for the prediction data and, thus, needs to know the column names. The target column is passed here only if it was also passed to the forecasting script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "upload_predictions_step = PythonScriptStep(\n",
    "    name=\"copy_predictions\",\n",
    "    script_name=\"copy_predictions.py\",\n",
    "    compute_target=compute,\n",
    "    source_directory='./scripts',\n",
    "    inputs=[output_dref, output_dir],\n",
    "    allow_reuse=False,\n",
    "    arguments=['--parallel_run_step_output', output_dir,\n",
    "               '--output_dir', output_dref,\n",
    "               '--target_column', 'Quantity',\n",
    "               '--timestamp_column', 'WeekStarting',\n",
    "               '--timeseries_id_columns', 'Store', 'Brand',]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallel_run_step, upload_predictions_step])\n",
    "run = experiment.submit(pipeline, tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following code to get a reference to a previous forecasting run\n",
    "#from azureml.pipeline.core import PipelineRun\n",
    "#run = PipelineRun(experiment, '<pipeline run id>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 View the results of the forecasting pipeline\n",
    "To see our forecasts, we download the *parallel_run_step.txt*, read the results into a dataframe, and visualize the predictions. Note that we could also download the results from the predictions container we created above.\n",
    "\n",
    "### 7.1 Download parallel_run_step.txt locally\n",
    "You need to wait until run that was submitted to Azure Machine Learning Compute Cluster is complete. You can monitor the run status in https://ml.azure.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def download_predictions(run, target_dir=None):\n",
    "    stitch_run = run.find_step_run(\"many-models-forecasting\")[0]\n",
    "    port_data = stitch_run.get_output_data('forecasting_output')\n",
    "    print(port_data)\n",
    "    port_data.download(target_dir, show_progress=True, overwrite=True)\n",
    "    step_hash = os.listdir(os.path.join(target_dir, 'azureml'))[0]\n",
    "    latest = sorted(Path(os.path.join(target_dir, 'azureml')).iterdir(), key=os.path.getmtime)\n",
    "    return  os.path.join(latest[-1], 'forecasting_output')\n",
    "\n",
    "file_path = download_predictions(run, 'output')\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Convert the file to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file_path + '/parallel_run_step.txt', sep=\" \", header=None)\n",
    "df.columns = ['WeekStarting', 'Predictions', 'Quantity', 'Store', 'Brand']\n",
    "df['WeekStarting'] = pd.to_datetime(df['WeekStarting'])\n",
    "# df['WeekStarting'] = [d.date() for d in pd.to_datetime(df['WeekStarting'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Visualize the predictions\n",
    "First, we look at the distribution of predicted quantities by brand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig = sns.violinplot(x=df['Brand'], y=df['Predictions'], data=df)\n",
    "fig.set_title('Predictions by Brand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we look at those predictions over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "week = df.groupby(['WeekStarting', 'Brand'])\n",
    "week = week['Predictions'].sum()\n",
    "week = pd.DataFrame(week.unstack(level=1))\n",
    "\n",
    "week.plot()\n",
    "plt.title('Total Predictions by Brand')\n",
    "plt.xticks(rotation=40)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Total Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we can trim the results to look at individual brands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "store = 1001\n",
    "df_1001 = df[df['Store'] == store]\n",
    "\n",
    "brands = df_1001.groupby(['WeekStarting','Brand'])\n",
    "brands= brands['Predictions'].sum()\n",
    "brands= pd.DataFrame(brands.unstack(level=1))\n",
    "\n",
    "brands.plot()\n",
    "plt.legend(loc='upper right', labels=brands.columns.values)\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Predictions for Store 1001')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Predicted Quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we produced forecasts on a test set, we can also examine forecast errors. The next plot displays the distribution of absolute percentage errors for each date in the testing period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the absolute percentage error for each forecast at each date\n",
    "# Warning: percentage errors are not defined if the actuals contain zero values\n",
    "df['APE'] = 100*np.abs((df['Quantity'].values - df['Predictions'].values) / df['Quantity'].values)\n",
    "\n",
    "fig = sns.boxplot(x='WeekStarting', y='APE', data=df)\n",
    "x_dates = df['WeekStarting'].dt.strftime('%Y-%m-%d')\n",
    "fig.set_xticklabels(x_dates, rotation=40, horizontalalignment='right')\n",
    "fig.set_title('Absolute Percentage Error Distributions Over All Stores and Brands')\n",
    "plt.gcf().set_size_inches(15.0, 6.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to see the error distributions broken down by store and brand. Here, the boxplots show the distribution of errors over time in the forecast period for each series in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.boxplot(x='Store', y='APE', hue='Brand', data=df)\n",
    "fig.set_title('Absolute Percentage Errors by Store and Brand')\n",
    "plt.gcf().set_size_inches(15.0, 6.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 Publish and schedule the pipeline (Optional)\n",
    "\n",
    "\n",
    "### 8.1 Publish the pipeline\n",
    "Once you have a pipeline you're happy with, you can publish a pipeline so you can call it programatically later on. See this [tutorial](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-your-first-pipeline#publish-a-pipeline) for additional information on publishing and calling pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_pipeline = pipeline.publish(name = 'forecast_many_models',\n",
    "#                                      description = 'forecast many models',\n",
    "#                                      version = '1',\n",
    "#                                      continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Schedule the pipeline\n",
    "You can also [schedule the pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-schedule-pipelines) to run on a time-based or change-based schedule. This could be used to automatically retrain models every month or based on another trigger such as data drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.pipeline.core import Schedule, ScheduleRecurrence\n",
    "    \n",
    "# training_pipeline_id = published_pipeline.id\n",
    "\n",
    "# recurrence = ScheduleRecurrence(frequency=\"Week\", interval=1, start_time=\"2020-01-01T09:00:00\")\n",
    "# recurring_schedule = Schedule.create(ws, name=\"forecasting_pipeline_recurring_schedule\", \n",
    "#                             description=\"Schedule Forecasting Pipeline to run on the first day of every week\",\n",
    "#                             pipeline_id=training_pipeline_id, \n",
    "#                             experiment_name=experiment.name, \n",
    "#                             recurrence=recurrence)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
