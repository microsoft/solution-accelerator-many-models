{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Pipeline\n",
    "---\n",
    "\n",
    "In this notebook we create a pipeline to forecast sales with the models we trained in the last step. The forecasting pipeline we'll set up is similar to the training pipeline in the last step so we'll keep the documentation light. For more details on the steps and functions refer to the last notebook.\n",
    "\n",
    "### Prerequisites\n",
    "At this point, you should have already:\n",
    "\n",
    "1. Created your AML Workspace using the [00_Setup_AML_Workspace notebook](../00_Setup_AML_Workspace.ipynb)\n",
    "2. Run [01_Data_Preparation.ipynb](01_Data_Preparation.ipynb) to setup your compute and create the dataset\n",
    "3. Run [02_Training_Pipeline.ipynb](02_Training_Pipeline.ipynb) to train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Connect to workspace and datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core import Datastore\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# set up datastores\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "print('Workspace Name: ' + ws.name, \n",
    "      'Azure Region: ' + ws.location, \n",
    "      'Subscription Id: ' + ws.subscription_id, \n",
    "      'Resource Group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Create an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'forecasting_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 3.0 Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "small_dataset = Dataset.get_by_name(ws, name='oj_data_small')\n",
    "small_dataset_input = small_dataset.as_named_input('forecast_10_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Create ParallelRunStep for the forecasting pipeline\n",
    "As we did with the training pipeline, we'll create a ParallelRunStep to parallelize our forecasting process. You'll notice this code is essentially the same as the last step except that we'll be parallelizing **forecast.py** rather than train.py.\n",
    "\n",
    "### 4.1 Configure environment for ParallelRunStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "forecast_env = Environment(name=\"many_models_environment\")\n",
    "forecast_conda_deps = CondaDependencies.create(pip_packages=['sklearn', 'joblib'])\n",
    "forecast_env.python.conda_dependencies = forecast_conda_deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Choose a compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "compute = AmlCompute(ws, \"cpucluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Set up ParallelRunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunConfig \n",
    "\n",
    "process_count_per_node = 8\n",
    "node_count = 5\n",
    "timeout = 180\n",
    "\n",
    "tags = {}\n",
    "tags['node_count'] = node_count\n",
    "tags['process_count_per_node'] = process_count_per_node\n",
    "tags['timeout'] = timeout\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./scripts',\n",
    "    entry_script='forecast.py',\n",
    "    mini_batch_size='1',\n",
    "    run_invocation_timeout=timeout, \n",
    "    error_threshold=10,\n",
    "    output_action='append_row', \n",
    "    environment=forecast_env, \n",
    "    process_count_per_node=process_count_per_node, \n",
    "    compute_target=compute, \n",
    "    node_count=node_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Set up ParallelRunStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep \n",
    "\n",
    "output_dir = PipelineData(name='forecasting_output', datastore=dstore)\n",
    "\n",
    "parallel_run_step = ParallelRunStep(\n",
    "    name=\"many-models-forecasting\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[small_dataset_input],\n",
    "    output=output_dir,\n",
    "    allow_reuse=False,\n",
    "    arguments=['--forecast_horizon', 8,\n",
    "              '--starting_date', '1992-10-01',\n",
    "              '--target_column', 'Quantity',\n",
    "              '--timestamp_column', 'WeekStarting',\n",
    "              '--model_type', 'lr',\n",
    "              '--date_freq', 'W-THU']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Create step to copy predictions\n",
    "\n",
    "The forecasting pipeline includes a second step that copies the predictions from *parallel_run_step.txt* to a CSV file in a separate container. While this step is simple, it demonstates how a step can be added to the pipeline to upload the predictions to a separate datastore or make additional transformations to the output.\n",
    "\n",
    "### 5.1 Create a data reference\n",
    "First, we create a datastore named **predictions** to hold the outputs of the pipeline and get a reference to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "output_dstore = Datastore.register_azure_blob_container(\n",
    "    workspace=ws, \n",
    "    datastore_name=\"predictions\",\n",
    "    container_name=\"predictions\",\n",
    "    account_name=dstore.account_name,\n",
    "    account_key=dstore.account_key,\n",
    "    create_if_not_exists=True\n",
    ")\n",
    "\n",
    "output_dref = DataReference(output_dstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create PythonScriptStep\n",
    "Next, we define the [PythonScriptStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py) and give it our newly create datastore as well as the location of the *parallel_run_step.txt*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "upload_predictions_step = PythonScriptStep(\n",
    "    name=\"copy_predictions\",\n",
    "    script_name=\"copy_predictions.py\",\n",
    "    compute_target=compute,\n",
    "    source_directory='./scripts',\n",
    "    inputs=[output_dref, output_dir],\n",
    "    allow_reuse=False,\n",
    "    arguments=['--parallel_run_step_output', output_dir,\n",
    "              '--output_dir', output_dref]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallel_run_step, upload_predictions_step])\n",
    "run = experiment.submit(pipeline, tags=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 View the results of the forecasting pipeline\n",
    "To see our forecasts, we download the *parallel_run_step.txt*, read the results into a dataframe, and visualize the predictions. Note that we could also download the results from the predictions container we created above.\n",
    "\n",
    "### 7.1 Download parallel_run_step.txt locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def download_predictions(run, target_dir=None):\n",
    "    stitch_run = run.find_step_run(\"many-models-forecasting\")[0]\n",
    "    port_data = stitch_run.get_output_data('forecasting_output')\n",
    "    print(port_data)\n",
    "    port_data.download(target_dir, show_progress=True)\n",
    "    step_hash = os.listdir(os.path.join(target_dir, 'azureml'))[0]\n",
    "    return  os.path.join(target_dir, 'azureml', step_hash, 'forecasting_output')\n",
    "\n",
    "file_path = download_predictions(run, 'output')\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Convert the file to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file_path + '/parallel_run_step.txt', sep=\" \", header=None)\n",
    "df.columns = ['WeekStarting', 'Predictions', 'Store', 'Brand']\n",
    "df['WeekStarting'] = [d.date() for d in pd.to_datetime(df['WeekStarting'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Visualize the predictions\n",
    "First, we look at the distribution of predicted quantities by brand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.violinplot(x=df['Brand'], y=df['Predictions'], data=df)\n",
    "fig.set_title('Predictions by Brand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we look at those predictions over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "week = df.groupby(['WeekStarting', 'Brand'])\n",
    "week = week['Predictions'].sum()\n",
    "week = pd.DataFrame(week.unstack(level=1))\n",
    "\n",
    "week.plot()\n",
    "plt.title('Total Predictions by Brand')\n",
    "plt.xticks(rotation=40)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Total Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we can trim the results to look at individual brands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "store = 'Store1001'\n",
    "df_1001 = df[df['Store'] == store]\n",
    "\n",
    "brands = df_1001.groupby(['WeekStarting','Brand'])\n",
    "brands= brands['Predictions'].sum()\n",
    "brands= pd.DataFrame(brands.unstack(level=1))\n",
    "\n",
    "brands.plot()\n",
    "plt.legend(loc='upper right', labels=brands.columns.values)\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Predictions for Store 1001')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Predicted Quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 Publish and schedule the pipeline (Optional)\n",
    "\n",
    "\n",
    "### 8.1 Publish the pipeline\n",
    "Once you have a pipeline you're happy with, you can publish a pipeline so you can call it programatically later on. See this [tutorial](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-your-first-pipeline#publish-a-pipeline) for additional information on publishing and calling pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_pipeline = pipeline.publish(name = 'forecast_many_models',\n",
    "#                                      description = 'forecast many models',\n",
    "#                                      version = '1',\n",
    "#                                      continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Schedule the pipeline\n",
    "You can also [schedule the pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-schedule-pipelines) to run on a time-based or change-based schedule. This could be used to automatically retrain models every month or based on another trigger such as data drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.pipeline.core import Schedule, ScheduleRecurrence\n",
    "    \n",
    "# training_pipeline_id = published_pipeline.id\n",
    "\n",
    "# recurrence = ScheduleRecurrence(frequency=\"Week\", interval=1, start_time=\"2020-01-01T09:00:00\")\n",
    "# recurring_schedule = Schedule.create(ws, name=\"forecasting_pipeline_recurring_schedule\", \n",
    "#                             description=\"Schedule Forecasting Pipeline to run on the first day of every week\",\n",
    "#                             pipeline_id=training_pipeline_id, \n",
    "#                             experiment_name=experiment.name, \n",
    "#                             recurrence=recurrence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
