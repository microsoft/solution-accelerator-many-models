{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "---\n",
    "\n",
    "This notebook walks you through all the necessary steps to configure your environment for this solution accelerator including:\n",
    "\n",
    "1. Connecting to your workspace and create a config.json (this can be skipped if running on a Notebook VM)\n",
    "2. Deploying a compute cluster for training and forecasting\n",
    "3. Creating and registering the dataset used in this accelerator\n",
    "\n",
    "### Prerequisites\n",
    "At this point, you should have created your AML workspace. If you haven't created one already, you can create one in step 1.1 below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Connect to workspace\n",
    "\n",
    "Connect this solution accelerator to your AML workspace. This step isn't necessary if you're using a Notebook VM.\n",
    "\n",
    "The following cell allows you to specify your workspace parameters. This cell uses the python method os.getenv to read values from environment variables which is useful for automation. If no environment variable exists, the parameters will be set to the specified default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"<my-subscriptoin-id>\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"<my-resource-group>\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"<my-workspace-name>\")\n",
    "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"westus2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id=subscription_id, \n",
    "                   resource_group=resource_group, \n",
    "                   workspace_name=workspace_name)\n",
    "    # write the details of the workspace to a config.json file\n",
    "    ws.write_config()\n",
    "    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\n",
    "except:\n",
    "    print(\"Workspace not accessible. Change your parameters or create a new workspace below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create workspace if needed\n",
    "If you don't have a workspace already, uncomment the lines below to create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the workspace using the specified parameters\n",
    "# ws = Workspace.create(\n",
    "#    name=workspace_name,\n",
    "#    subscription_id=subscription_id,\n",
    "#    resource_group=resource_group, \n",
    "#    location=workspace_region,\n",
    "#    create_resource_group=True,\n",
    "#    sku='basic',\n",
    "#    exist_ok=True\n",
    "#)\n",
    "#ws.get_details()\n",
    "\n",
    "# write the details of the workspace to a configuration file in the parent folder\n",
    "#ws.write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Create compute\n",
    "\n",
    "In this step we create an compute cluster that will be used for the training and forecasting pipelines. This is a one-time set up so you won't need to re-run this in future notebooks.\n",
    "\n",
    "We create a STANDARD_D13_V2 compute cluster. D-series VMs are used for tasks that require higher compute power and temporary disk performance. This [page](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs) will gives you more information on VM sizes to help you decide which will best fit your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D13_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=5)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Create dataset\n",
    "\n",
    "This solution accelerator uses simulated orange juice sales data from [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/) to walk you through the process of training many models on Azure Machine Learning. You can learn more about the dataset [here](https://azure.microsoft.com/en-us/services/open-datasets/catalog/sample-oj-sales-simulated/). The full dataset includes simulared sales for 3,991 stores with 3 orange juice brands each thus allowing 11,973 models to be trained to showcase the power of the many models pattern.\n",
    "\n",
    "We'll start by downloading the first 10 files but you can easily edit the code below to train all 11,973 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade azureml-opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "from azureml.opendatasets import OjSalesSimulated\n",
    "\n",
    "# Pull all of the data\n",
    "oj_sales_files = OjSalesSimulated.get_file_dataset()\n",
    "\n",
    "# Pull only the first 10 files\n",
    "oj_sales_files_small = OjSalesSimulated.get_file_dataset().take(10)\n",
    "\n",
    "# Create a folder to download\n",
    "target_path = 'oj_sales_data' \n",
    "if not os.path.exists(target_path):\n",
    "    os.mkdir(target_path)\n",
    "\n",
    "# Download the data\n",
    "oj_sales_files_small.download(target_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create and register a [dataset](https://docs.microsoft.com/en-us/azure/machine-learning/concept-data#datasets) in Azure Machine Learning. \n",
    "\n",
    "Using a [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py) is currently the best way to take advantage of the many models pattern so we create a FileDataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Upload the data\n",
    "datastore.upload(src_dir = target_path,\n",
    "                target_path = target_path,\n",
    "                overwrite = True)\n",
    "\n",
    "# Create a file dataset\n",
    "path_on_datastore = datastore.path(target_path)\n",
    "ds = Dataset.File.from_files(path=path_on_datastore, validate=False)\n",
    "\n",
    "# Register the file dataset\n",
    "dataset_name = 'oj_data_small'\n",
    "ds.register(ws, dataset_name, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've set up your workspace and created a dataset, move on to 01_Training_Pipeline.ipynb to train and score the models."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
