{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/manymodels/01_Data_Preparation/01_Data_Preparation.png)"
   ]
  },
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01b Environment Setup\n",
    "This notebook uses simulated orange juice sales data to walk you through the process of training many models on Azure Machine Learning using Automated ML. \n",
    "\n",
    "The time series data used in this example was simulated based on the University of Chicago's Dominick's Finer Foods dataset which featured two years of sales of 3 different orange juice brands for individual stores. The full simulated dataset includes 3,991 stores with 3 orange juice brands each thus allowing 11,973 models to be trained to showcase the power of the many models pattern.\n",
    "\n",
    "  \n",
    "In this notebook, two datasets will be created: one with all 11,973 files and one with only 10 files that can be used to quickly test and debug. For each dataset, you'll be walked through the process of:\n",
    "\n",
    "1. Downloading the data from Azure Open Datasets\n",
    "2. Uploading the data to Azure Blob Storage\n",
    "3. Registering a File Dataset to the Workspace\n",
    "\n",
    "## Prerequisites\n",
    "You will need to setup your workspace using the configuration notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Connect to your Workspace and Datastore\n",
    "In the configuration notebook you created a [Workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py). We are going to use that enviroment to register the data. You also set up the Datastore which in this example is a container in Blob storage where we will store the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config() \n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Take a look at Workspace\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Download the data from Azure Open Datasets\n",
    "To download the data, import OjSalesSimulated from Azure Open Datasets. Two datasets are being created: one with all 11,973 files and one with 10 files but this can be customized based on your preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you have to install OjSalesSimulated, run the following command and restart the kernal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install azureml-opendatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> To use your own data, create a local folder with each group as a separate file. The data has to be presplit into different files by group and for OJ data, it was already split by Store and Brand. For timeseries, the groups must not split up individual timeseries. That is, each group must contain one or more whole time-series</b>\n",
    "Then use that folder and directory in section 3.0 to upload your data to the Datastore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.opendatasets import OjSalesSimulated\n",
    "\n",
    "# Pull all of the data\n",
    "oj_sales_files = OjSalesSimulated.get_file_dataset()\n",
    "\n",
    "# Pull the first 10 files\n",
    "oj_sales_files_small = OjSalesSimulated.get_file_dataset().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the folders that the data will be downloaded to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "oj_sales_path = \"oj_sales_data\"\n",
    "if not os.path.exists(oj_sales_path):\n",
    "    os.mkdir(oj_sales_path)\n",
    "    \n",
    "oj_sales_path_small = \"oj_sales_data_small\"\n",
    "if not os.path.exists(oj_sales_path_small):\n",
    "    os.mkdir(oj_sales_path_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, download the files to the folder you created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oj_sales_files.download(oj_sales_path, overwrite=True)\n",
    "oj_sales_files_small.download(oj_sales_path_small, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Split the data and upload the files to your Datastore\n",
    "We will split the data so that we can use part of the data for inferencing. For the current dataset, we will be splitting on time column ('WeekStarting') before and after '1992-5-28' .\n",
    "\n",
    "You are now ready to load the historical orange juice sales data. To create the [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py) needed for the ParallelRunStep, you first need to upload the csv files to your blob datastore.\n",
    "\n",
    "Please *note* that this will take time as we process 11k files to get the train and test data.  The following scripts will create 'upload_train_data' and 'upload_test_data' folders under the 'oj_sales_data' and 'oj_sales_data_small' folders. The data will then be uploaded to the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helper import  split_data_upload_to_datastore\n",
    "time_column_name = 'WeekStarting'\n",
    "target_date = '1992-5-28'\n",
    "\n",
    "\n",
    "target_path = 'oj_sales_data' \n",
    "target_inference_path = 'oj_sales_inference'\n",
    "split_data_upload_to_datastore(data_path = oj_sales_path,\n",
    "                               column_name = time_column_name,\n",
    "                               date = target_date,\n",
    "                               datastore = datastore,\n",
    "                               train_ds_target_path = target_path,\n",
    "                               test_ds_target_path = target_inference_path)\n",
    "\n",
    "\n",
    "target_path_small = 'oj_sales_data_small'\n",
    "target_inference_path_small = 'oj_sales_inference_small'\n",
    "split_data_upload_to_datastore(data_path = oj_sales_path_small,\n",
    "                               column_name = time_column_name,\n",
    "                               date = target_date,\n",
    "                               datastore = datastore,\n",
    "                               train_ds_target_path = target_path_small,\n",
    "                               test_ds_target_path = target_inference_path_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Create the FileDatasets \n",
    "\n",
    "Now that the files exist in the datastore, [FileDatasets](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py) can be created. Datasets in Azure Machine Learning are references to specific data in a Datastore.  We are using FileDatasets since we are storing each group as a separate file. This will help to separate the data needed for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "ds_name = 'oj_data'\n",
    "input_ds = Dataset.File.from_files(path=datastore.path(target_path + '/'), validate=False)\n",
    "\n",
    "inference_name = 'oj_inference'\n",
    "inference_ds = Dataset.File.from_files(path=datastore.path(target_inference_path + '/'), validate=False)\n",
    "\n",
    "ds_name_small = 'oj_data_small'\n",
    "input_ds_small = Dataset.File.from_files(path=datastore.path(target_path_small + '/'), validate=False)\n",
    "\n",
    "inference_name_small = 'oj_inference_small'\n",
    "inference_ds_small = Dataset.File.from_files(path=datastore.path(target_inference_path_small + '/'), validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Register the FileDataSets to the Workspace \n",
    "Finally, register the dataset to your Workspace so it can be called as an input into the training pipeline in the next notebook. We will use the inference dataset as part of the forecasting pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ds.register(ws, ds_name, create_new_version=True)\n",
    "inference_ds.register(ws, inference_name, create_new_version=True)\n",
    "\n",
    "input_ds_small.register(ws, ds_name_small, create_new_version=True)\n",
    "inference_ds_small.register(ws, inference_name_small, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Call the Registered dataset *(Optional)*\n",
    "After registering the data, it can be easily called using the command below. This is how the datasets will be accessed in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oj_ds = Dataset.get_by_name(ws, name = ds_name_small)\n",
    "oj_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also download the data from the dataset in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oj_ds.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 Delete the local files *(Optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(oj_sales_path)\n",
    "shutil.rmtree(oj_sales_path_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "Now that you have created your dataset, you are ready to move to the Training Notebook to create models. "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "deeptim"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
